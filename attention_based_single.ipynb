{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drago\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, AutoModel, AutoConfig\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from model import AttentionModelSingle\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/drago/.cache/huggingface/datasets/csv/default-e9382578803c537b/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "ds = Dataset.from_csv('data/mental_health.csv')\n",
    "# Select the amount of data to drop\n",
    "to_keep = 0.0\n",
    "if to_keep:\n",
    "    ds = ds.train_test_split(test_size=to_keep)['train']\n",
    "# Split the data into train, test, and validation\n",
    "all_parts = ds.train_test_split(test_size=0.2,shuffle=True)\n",
    "test_valid = all_parts['test'].train_test_split(test_size=0.5)\n",
    "data = DatasetDict({\n",
    "    'train': all_parts['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\drago\\OneDrive\\Documents\\nlp-mental-health\\model.py:117: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.att_token_param)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "checkpoint_base = 'distilbert-base-uncased'\n",
    "\n",
    "model = AttentionModelSingle(checkpoint_base, 2, freeze_base=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'],padding=True,truncation=True)\n",
    "\n",
    "data_tokenized = data.map(tokenize,batched=True)\n",
    "\n",
    "# Remove the columns we don't need\n",
    "new_dataset=data_tokenized.remove_columns('text')\n",
    "new_dataset=new_dataset.rename_column('label','labels')\n",
    "new_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drago\\miniconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the dataloader to specify how the data is batched and passed into the model for training\n",
    "train_loader = DataLoader(new_dataset['train'], batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(new_dataset['valid'], batch_size=16)\n",
    "\n",
    "# Create the optimizer, which specifies the parameters to update and how to update them\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Val Loss: 0.0009493142133578658\n"
     ]
    }
   ],
   "source": [
    "lowest_loss = np.inf\n",
    "# One epoch is one pass through the entire dataset\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    # Iterate over the batches of data. The dataloader will return the batches\n",
    "    for batch in tqdm(train_loader, leave=False):\n",
    "        # Clear the gradients from the previous iteration\n",
    "        optim.zero_grad()\n",
    "        # Move the batch of inputs to the GPU/CPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        # the attention mask is used to ignore the padding tokens, i.e. tokens that are not part of the sentence\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Move the labels to the GPU/CPU\n",
    "        labels = batch['labels'].to(device)\n",
    "        # Forward pass the inputs through the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        # Get the loss from the first element of the outputs tuple\n",
    "        loss = outputs\n",
    "        # Backpropagate the loss to update the model's parameters\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optim.step()\n",
    "\n",
    "    # Print the training and validation loss\n",
    "    model.eval()  # handle drop-out/batch norm layers\n",
    "    val_loss = 0\n",
    "    train_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            # the attention mask is used to ignore the padding tokens, i.e. tokens that are not part of the sentence\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # Move the labels to the GPU/CPU\n",
    "            labels = batch['labels'].to(device)\n",
    "            # Forward pass the inputs through the model\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            # Get the loss from the first element of the outputs tuple\n",
    "            val_loss += outputs\n",
    "        # total loss - divide by number of batches\n",
    "        val_loss = loss / len(val_loader)\n",
    "    \n",
    "    if val_loss < lowest_loss:\n",
    "        lowest_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'val_loss': lowest_loss,\n",
    "            }, \"model.pt\")\n",
    "    \n",
    "    print(\"Epoch: {}, Val Loss: {}\".format(epoch+1, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AttentionModelSingle' object has no attribute 'test_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\drago\\OneDrive\\Documents\\nlp-mental-health\\attention_based_single.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/drago/OneDrive/Documents/nlp-mental-health/attention_based_single.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Set the model to evaluation mode. So we don't update the weights\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/drago/OneDrive/Documents/nlp-mental-health/attention_based_single.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/drago/OneDrive/Documents/nlp-mental-health/attention_based_single.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mtest_mode()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/drago/OneDrive/Documents/nlp-mental-health/attention_based_single.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/drago/OneDrive/Documents/nlp-mental-health/attention_based_single.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# For simplicity we'll evaluate passing all the samples at once\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\drago\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AttentionModelSingle' object has no attribute 'test_mode'"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode. So we don't update the weights\n",
    "model.eval()\n",
    "predictions = np.array([])\n",
    "\n",
    "# For simplicity we'll evaluate passing all the samples at once\n",
    "test_loader = DataLoader(new_dataset['test'], batch_size=100, shuffle=False)\n",
    "\n",
    "# Get the scores on the test set\n",
    "with torch.no_grad():\n",
    "    # Pass through training set\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Get the logits from the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,loss=False)\n",
    "        # Get the predictions by taking the argmax of the logits\n",
    "\n",
    "        #predictions = np.append(predictions, torch.argmax(outputs, dim=1).tolist())\n",
    "        for i in outputs:\n",
    "          if i<0:\n",
    "            predictions = np.append(predictions,0)\n",
    "          else:\n",
    "            predictions =  np.append(predictions,1)\n",
    "\n",
    "\n",
    "# Print accuracy\n",
    "test_label = new_dataset['test']['labels'].tolist()\n",
    "acc = accuracy_score(test_label, predictions.tolist())\n",
    "f1 = f1_score(test_label, predictions.tolist())\n",
    "prec = precision_score(test_label, predictions.tolist())\n",
    "rec = recall_score(test_label, predictions.tolist())\n",
    "\n",
    "print(\"Accuracy: {}, F1: {}, Precision: {}, Recall: {}\".format(acc, f1, prec, rec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
