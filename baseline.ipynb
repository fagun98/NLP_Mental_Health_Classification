{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/drago/.cache/huggingface/datasets/csv/default-e9382578803c537b/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "ds = Dataset.from_csv('data/mental_health.csv')\n",
    "# Select the amount of data to drop\n",
    "to_keep = 0.0\n",
    "if to_keep:\n",
    "    ds = ds.train_test_split(test_size=to_keep)['train']\n",
    "# Split the data into train, test, and validation\n",
    "all_parts = ds.train_test_split(test_size=0.2,shuffle=True)\n",
    "test_valid = all_parts['test'].train_test_split(test_size=0.5)\n",
    "data = DatasetDict({\n",
    "    'train': all_parts['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2).to(device)\n",
    "\n",
    "freeze_weights = True\n",
    "\n",
    "if freeze_weights:\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' not in name:\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\drago\\.cache\\huggingface\\datasets\\csv\\default-e9382578803c537b\\0.0.0\\cache-cf49302386046d32.arrow\n",
      "Loading cached processed dataset at C:\\Users\\drago\\.cache\\huggingface\\datasets\\csv\\default-e9382578803c537b\\0.0.0\\cache-937c65105e3adce5.arrow\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'],padding=True,truncation=True)\n",
    "\n",
    "data_tokenized = data.map(tokenize,batched=True)\n",
    "\n",
    "# Remove the columns we don't need\n",
    "new_dataset=data_tokenized.remove_columns('text')\n",
    "new_dataset=new_dataset.rename_column('label','labels')\n",
    "new_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drago\\miniconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the dataloader to specify how the data is batched and passed into the model for training\n",
    "train_loader = DataLoader(new_dataset['train'], batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(new_dataset['valid'], batch_size=16)\n",
    "\n",
    "# Create the optimizer, which specifies the parameters to update and how to update them\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.0002048851310973987, Val Loss: 0.001637910259887576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.0003730198659468442, Val Loss: 0.002982027130201459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 0.0001945940311998129, Val Loss: 0.0015556401340290904\n"
     ]
    }
   ],
   "source": [
    "lowest_loss = np.inf\n",
    "# One epoch is one pass through the entire dataset\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    # Iterate over the batches of data. The dataloader will return the batches\n",
    "    for batch in tqdm(train_loader, leave=False):\n",
    "        # Clear the gradients from the previous iteration\n",
    "        optim.zero_grad()\n",
    "        # Move the batch of inputs to the GPU/CPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        # the attention mask is used to ignore the padding tokens, i.e. tokens that are not part of the sentence\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Move the labels to the GPU/CPU\n",
    "        labels = batch['labels'].to(device)\n",
    "        # Forward pass the inputs through the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        # Get the loss from the first element of the outputs tuple\n",
    "        loss = outputs[0]\n",
    "        # Backpropagate the loss to update the model's parameters\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optim.step()\n",
    "\n",
    "    # Print the training and validation loss\n",
    "    model.eval()  # handle drop-out/batch norm layers\n",
    "    val_loss = 0\n",
    "    train_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            # the attention mask is used to ignore the padding tokens, i.e. tokens that are not part of the sentence\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # Move the labels to the GPU/CPU\n",
    "            labels = batch['labels'].to(device)\n",
    "            # Forward pass the inputs through the model\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            # Get the loss from the first element of the outputs tuple\n",
    "            val_loss += outputs[0]\n",
    "        # total loss - divide by number of batches\n",
    "        val_loss = loss / len(val_loader)\n",
    "    \n",
    "    if val_loss < lowest_loss:\n",
    "        lowest_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'val_loss': lowest_loss,\n",
    "            }, \"model.pt\")\n",
    "    \n",
    "    print(\"Epoch: {}, Val Loss: {}\".format(epoch+1, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8934953538241601, F1: 0.8934953538241601, Precision: 0.8668515950069348, Recall: 0.9218289085545722\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode. So we don't update the weights\n",
    "model.eval()\n",
    "predictions = np.array([])\n",
    "\n",
    "# For simplicity we'll evaluate passing all the samples at once\n",
    "test_loader = DataLoader(new_dataset['test'], batch_size=100, shuffle=False)\n",
    "\n",
    "# Get the scores on the test set\n",
    "with torch.no_grad():\n",
    "    # Pass through training set\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Get the logits from the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        # Get the predictions by taking the argmax of the logits\n",
    "        predictions = np.append(predictions, torch.argmax(outputs[0], dim=1).tolist())\n",
    "\n",
    "\n",
    "# Print accuracy\n",
    "test_label = new_dataset['test']['labels'].tolist()\n",
    "acc = accuracy_score(test_label, predictions.tolist())\n",
    "f1 = f1_score(test_label, predictions.tolist())\n",
    "prec = precision_score(test_label, predictions.tolist())\n",
    "rec = recall_score(test_label, predictions.tolist())\n",
    "\n",
    "print(\"Accuracy: {}, F1: {}, Precision: {}, Recall: {}\".format(acc, f1, prec, rec))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
